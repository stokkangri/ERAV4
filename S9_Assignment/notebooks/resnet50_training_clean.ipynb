{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ResNet50 Training on ImageNet\n",
    "\n",
    "This notebook trains ResNet50 on ImageNet with the following configuration:\n",
    "- Model: ResNet50 (imported from `models/model_resnet50.py`)\n",
    "- Batch size: 128 (increased from ResNet152's 48)\n",
    "- Model name: `resnet_50_sgd1`\n",
    "- Architecture: ResNet50 with [3, 4, 6, 3] blocks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "import os\n",
    "import time\n",
    "\n",
    "# Import PyTorch libraries\n",
    "import torch\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "import torchvision.datasets as datasets\n",
    "from torch.utils.data import DataLoader\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "\n",
    "print(\"Libraries imported - ready to use PyTorch\", torch.__version__)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup Parameters and Device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Helper function\n",
    "def show_image(image, label):\n",
    "    image = image.permute(1, 2, 0)\n",
    "    plt.imshow(image.squeeze())\n",
    "    plt.title(f'Label: {label}')\n",
    "    plt.show()\n",
    "\n",
    "# Device setup\n",
    "device = (\n",
    "    \"cuda\"\n",
    "    if torch.cuda.is_available()\n",
    "    else \"mps\"\n",
    "    if torch.backends.mps.is_available()\n",
    "    else \"cpu\"\n",
    ")\n",
    "print(f\"Using {device} device\")\n",
    "\n",
    "# Resume training option\n",
    "resume_training = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training parameters\n",
    "class Params:\n",
    "    def __init__(self):\n",
    "        self.batch_size = 128  # Changed from 48 to 128 for ResNet50\n",
    "        self.name = \"resnet_50_sgd1\"  # Changed from resnet_152_sgd1\n",
    "        self.workers = 4\n",
    "        self.lr = 0.002\n",
    "        self.momentum = 0.9\n",
    "        self.weight_decay = 1e-4\n",
    "        self.lr_step_size = 30\n",
    "        self.lr_gamma = 0.1\n",
    "\n",
    "    def __repr__(self):\n",
    "        return str(self.__dict__)\n",
    "\n",
    "    def __eq__(self, other):\n",
    "        return self.__dict__ == other.__dict__\n",
    "\n",
    "params = Params()\n",
    "print(params)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data paths\n",
    "training_folder_name = '/home/xpz1/Downloads/imagenet-object-localization-challenge/ILSVRC/Data/CLS-LOC/train'\n",
    "val_folder_name = '/home/xpz1/Downloads/imagenet-object-localization-challenge/ILSVRC/Data/CLS-LOC/val'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training data loader\n",
    "train_transformation = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.RandomResizedCrop(224, interpolation=transforms.InterpolationMode.BILINEAR, antialias=True),\n",
    "    transforms.RandomHorizontalFlip(0.5),\n",
    "    transforms.Normalize(mean=[0.485, 0.485, 0.406], std=[0.229, 0.224, 0.225])\n",
    "])\n",
    "\n",
    "train_dataset = torchvision.datasets.ImageFolder(\n",
    "    root=training_folder_name,\n",
    "    transform=train_transformation\n",
    ")\n",
    "train_sampler = torch.utils.data.RandomSampler(train_dataset)\n",
    "train_loader = torch.utils.data.DataLoader(\n",
    "    train_dataset,\n",
    "    batch_size=params.batch_size,\n",
    "    sampler=train_sampler,\n",
    "    num_workers=params.workers,\n",
    "    pin_memory=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Validation data loader\n",
    "val_transformation = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Resize(size=256, antialias=True),\n",
    "    transforms.CenterCrop(224),\n",
    "    transforms.Normalize(mean=[0.485, 0.485, 0.406], std=[0.229, 0.224, 0.225])\n",
    "])\n",
    "\n",
    "val_dataset = torchvision.datasets.ImageFolder(\n",
    "    root=val_folder_name,\n",
    "    transform=val_transformation\n",
    ")\n",
    "\n",
    "val_loader = torch.utils.data.DataLoader(\n",
    "    val_dataset,\n",
    "    batch_size=128,  # Increased to match training batch size\n",
    "    num_workers=params.workers,\n",
    "    shuffle=False,\n",
    "    pin_memory=True\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define Training and Testing Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from math import sqrt\n",
    "\n",
    "def train(dataloader, model, loss_fn, optimizer, epoch, writer):\n",
    "    size = len(dataloader.dataset)\n",
    "    model.train()\n",
    "    start0 = time.time()\n",
    "    start = time.time()\n",
    "\n",
    "    # Add tracking variables\n",
    "    total_loss = 0\n",
    "    correct = 0\n",
    "\n",
    "    for batch, (X, y) in enumerate(dataloader):\n",
    "        X, y = X.to(device), y.to(device)\n",
    "        \n",
    "        # Compute prediction error\n",
    "        pred = model(X)\n",
    "        loss = loss_fn(pred, y)\n",
    "        \n",
    "        # Track metrics\n",
    "        total_loss += loss.item()\n",
    "        correct += (pred.argmax(1) == y).type(torch.float).sum().item()\n",
    "\n",
    "        # Backpropagation\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        batch_size = len(X)\n",
    "        if batch % 100 == 0:\n",
    "            loss, current = loss.item(), (batch + 1) * batch_size\n",
    "            print(f\"loss: {loss:>7f}  [{current:>5d}/{size:>5d}], {(current/size * 100):>4f}%\")\n",
    "            step = epoch * size + current\n",
    "            writer.add_scalar('training loss', loss, step)\n",
    "            new_start = time.time()\n",
    "            delta = new_start - start\n",
    "            start = new_start\n",
    "            if batch != 0:\n",
    "                print(\"Done in \", delta, \" seconds\")\n",
    "                remaining_steps = size - current\n",
    "                speed = 100 * batch_size / delta\n",
    "                remaining_time = remaining_steps / speed\n",
    "                print(\"Remaining time (seconds): \", remaining_time)\n",
    "        optimizer.zero_grad()\n",
    "    print(\"Entire epoch done in \", time.time() - start0, \" seconds\")\n",
    "    # Return metrics\n",
    "    avg_loss = total_loss / len(dataloader)\n",
    "    accuracy = 100. * correct / size\n",
    "    return avg_loss, accuracy\n",
    "\n",
    "def test(dataloader, model, loss_fn, epoch, writer, train_dataloader, calc_acc5=False):\n",
    "    size = len(dataloader.dataset)\n",
    "    num_batches = len(dataloader)\n",
    "    model.eval()\n",
    "    test_loss, correct, correct_top5 = 0, 0, 0\n",
    "    with torch.no_grad():\n",
    "        for X, y in dataloader:\n",
    "            X, y = X.to(device), y.to(device)\n",
    "            pred = model(X)\n",
    "            test_loss += loss_fn(pred, y).item()\n",
    "            correct += (pred.argmax(1) == y).type(torch.float).sum().item()\n",
    "            if calc_acc5:\n",
    "                _, pred_top5 = pred.topk(5, 1, largest=True, sorted=True)\n",
    "                correct_top5 += pred_top5.eq(y.view(-1, 1).expand_as(pred_top5)).sum().item()\n",
    "    test_loss /= num_batches\n",
    "    step = epoch * len(train_dataloader.dataset)\n",
    "    if writer != None:\n",
    "        writer.add_scalar('test loss', test_loss, step)\n",
    "    correct /= size\n",
    "    correct_top5 /= size\n",
    "    if writer != None:\n",
    "        writer.add_scalar('test accuracy', 100*correct, step)\n",
    "        if calc_acc5:\n",
    "            writer.add_scalar('test accuracy5', 100*correct_top5, step)\n",
    "    print(f\"Test Error: \\n Accuracy: {(100*correct):>0.1f}%, Avg loss: {test_loss:>8f} \\n\")\n",
    "    if calc_acc5:\n",
    "        print(f\"Test Error: \\n Accuracy-5: {(100*correct_top5):>0.1f}%, Avg loss: {test_loss:>8f} \\n\")\n",
    "    \n",
    "    return test_loss, correct * 100, correct_top5 * 100  # Return as percentages"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import ResNet50 Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import ResNet50 from our model file\n",
    "import sys\n",
    "sys.path.append('..')  # Add parent directory to path\n",
    "from models.model_resnet50 import resnet50\n",
    "\n",
    "print(\"ResNet50 model imported successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create ResNet50 Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clear GPU memory\n",
    "import gc\n",
    "torch.cuda.empty_cache()\n",
    "gc.collect()\n",
    "\n",
    "# Create ResNet50 model using our imported function\n",
    "device = \"cuda\"\n",
    "model = resnet50(num_classes=1000).to(device)\n",
    "\n",
    "print(\"Model: ResNet50\")\n",
    "print(f\"Total parameters: {sum(p.numel() for p in model.parameters()):,}\")\n",
    "print(f\"\\nUsing batch size: {params.batch_size}\")\n",
    "print(\"ResNet50 uses ~3x less memory than ResNet152, allowing larger batch sizes\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Learning Rate Finder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use the same LR finder code as ResNet152\n",
    "from torch_lr_finder import LRFinder\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from scipy.ndimage import gaussian_filter1d\n",
    "\n",
    "# Run LR finder\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.SGD(model.parameters(), lr=1e-7, weight_decay=1e-4)\n",
    "lr_finder = LRFinder(model, optimizer, criterion, device=\"cuda\")\n",
    "lr_finder.range_test(train_loader, end_lr=0.02, num_iter=5000, smooth_f=0.1)\n",
    "\n",
    "# Extract and analyze\n",
    "lrs = lr_finder.history[\"lr\"]\n",
    "losses = lr_finder.history[\"loss\"]\n",
    "smoothed_losses = gaussian_filter1d(losses, sigma=50)\n",
    "gradients = np.gradient(smoothed_losses, np.log(lrs))\n",
    "min_grad_idx = np.argmin(gradients)\n",
    "optimal_lr = lrs[min_grad_idx]\n",
    "\n",
    "# Plot\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(lrs, losses, label=\"Original Loss\", alpha=0.5)\n",
    "plt.plot(lrs, smoothed_losses, label=\"Smoothed Loss\", color=\"red\")\n",
    "plt.scatter(optimal_lr, smoothed_losses[min_grad_idx], color=\"blue\", \n",
    "            label=f\"Steepest Drop LR: {optimal_lr:.2e}\", zorder=5)\n",
    "plt.xscale(\"log\")\n",
    "plt.xlabel(\"Learning Rate\")\n",
    "plt.ylabel(\"Loss\")\n",
    "plt.title(\"Learning Rate Finder with Steepest Drop Marked\")\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.show()\n",
    "\n",
    "print(f\"Learning rate with steepest drop in loss: {optimal_lr:.2e}\")\n",
    "lr_finder.reset()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set learning rate and create optimizer/scheduler\n",
    "params.lr = optimal_lr # 0.0001  # Or use optimal_lr from LR finder\n",
    "\n",
    "loss_fn = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.SGD(model.parameters(), \n",
    "                            lr=params.lr, momentum=params.momentum, weight_decay=params.weight_decay)\n",
    "\n",
    "# OneCycleLR scheduler\n",
    "# Note: steps_per_epoch will be different with batch_size=128\n",
    "steps_per_epoch = len(train_loader)\n",
    "print(f\"Steps per epoch with batch_size={params.batch_size}: {steps_per_epoch}\")\n",
    "\n",
    "lr_scheduler = torch.optim.lr_scheduler.OneCycleLR(\n",
    "    optimizer, max_lr=0.002, total_steps=None, epochs=100, \n",
    "    steps_per_epoch=steps_per_epoch, pct_start=0.3, anneal_strategy='cos', \n",
    "    cycle_momentum=True, base_momentum=0.85, max_momentum=0.95, \n",
    "    div_factor=10.0, final_div_factor=1000.0, \n",
    "    three_phase=False, last_epoch=-1, verbose='deprecated'\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Resume Training Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Resume from checkpoint if available\n",
    "start_epoch = 0\n",
    "checkpoint_path = os.path.join(\"checkpoints\", params.name, f\"checkpoint.pth\")\n",
    "\n",
    "if resume_training and os.path.exists(checkpoint_path):\n",
    "    checkpoint = torch.load(checkpoint_path)\n",
    "    model.load_state_dict(checkpoint[\"model\"])\n",
    "    start_epoch = checkpoint[\"epoch\"] + 1\n",
    "    optimizer.load_state_dict(checkpoint[\"optimizer\"])\n",
    "    lr_scheduler.load_state_dict(checkpoint[\"lr_scheduler\"])\n",
    "    assert params == checkpoint[\"params\"]\n",
    "    print(f\"Resumed from epoch {start_epoch}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training Loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Initialize Training History\n",
    "# Training history for plotting\n",
    "history = {\n",
    "    'train_loss': [],\n",
    "    'train_acc': [],\n",
    "    'val_loss': [],\n",
    "    'val_acc': [],\n",
    "    'val_acc_top5': [],\n",
    "    'lr': []\n",
    "}\n",
    "\n",
    "# Best model tracking\n",
    "best_val_acc = 0\n",
    "best_epoch = 0\n",
    "\n",
    "# Setup tensorboard\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "from pathlib import Path\n",
    "\n",
    "Path(os.path.join(\"checkpoints\", params.name)).mkdir(parents=True, exist_ok=True)\n",
    "writer = SummaryWriter('runs/' + params.name)\n",
    "\n",
    "# Initial validation\n",
    "test(val_loader, model, loss_fn, epoch=0, writer=writer, train_dataloader=train_loader, calc_acc5=True)\n",
    "\n",
    "# Training loop\n",
    "for epoch in range(start_epoch, 100):\n",
    "    print(f\"Running Epoch:{epoch} \")\n",
    "    # Get training metrics\n",
    "    train_loss, train_acc = train(train_loader, model, loss_fn, optimizer, epoch=epoch, writer=writer)\n",
    "    \n",
    "    checkpoint = {\n",
    "        \"model\": model.state_dict(),\n",
    "        \"optimizer\": optimizer.state_dict(),\n",
    "        \"lr_scheduler\": lr_scheduler.state_dict(),\n",
    "        \"epoch\": epoch,\n",
    "        \"params\": params\n",
    "    }\n",
    "    torch.save(checkpoint, os.path.join(\"checkpoints\", params.name, f\"model_{epoch}.pth\"))\n",
    "    torch.save(checkpoint, os.path.join(\"checkpoints\", params.name, f\"checkpoint.pth\"))\n",
    "    \n",
    "    lr_scheduler.step()\n",
    "    # Get validation metrics\n",
    "    val_loss, val_acc, val_acc_top5 = test(val_loader, model, loss_fn, epoch + 1, writer, \n",
    "                                           train_dataloader=train_loader, calc_acc5=True)\n",
    "    \n",
    "    # Update history\n",
    "    current_lr = optimizer.param_groups[0]['lr']\n",
    "    history['train_loss'].append(train_loss)\n",
    "    history['train_acc'].append(train_acc)\n",
    "    history['val_loss'].append(val_loss)\n",
    "    history['val_acc'].append(val_acc)\n",
    "    history['val_acc_top5'].append(val_acc_top5)\n",
    "    history['lr'].append(current_lr)\n",
    "    \n",
    "    # Track best model\n",
    "    if val_acc > best_val_acc:\n",
    "        best_val_acc = val_acc\n",
    "        best_epoch = epoch\n",
    "        torch.save(checkpoint, os.path.join(\"checkpoints\", params.name, f\"best_model.pth\"))\n",
    "    \n",
    "    # Print epoch summary\n",
    "    print(f\"\\nEpoch {epoch+1}/100 Summary:\")\n",
    "    print(f\"  Train Loss: {train_loss:.4f}, Train Acc: {train_acc:.2f}%\")\n",
    "    print(f\"  Val Loss: {val_loss:.4f}, Val Acc: {val_acc:.2f}%\")\n",
    "    print(f\"  Val Top-5 Acc: {val_acc_top5:.2f}%\")\n",
    "    print(f\"  Learning Rate: {current_lr:.2e}\")\n",
    "    print(f\"  Best Val Acc: {best_val_acc:.2f}% (Epoch {best_epoch+1})\")\n",
    "    print(\"=\" * 50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Plot Training Progress\n",
    "# Plot training curves\n",
    "fig, axes = plt.subplots(2, 2, figsize=(12, 10))\n",
    "\n",
    "epochs = range(1, len(history['train_loss']) + 1)\n",
    "\n",
    "# Loss plot\n",
    "axes[0, 0].plot(epochs, history['train_loss'], 'b-', label='Train')\n",
    "axes[0, 0].plot(epochs, history['val_loss'], 'r-', label='Val')\n",
    "axes[0, 0].set_title('Loss')\n",
    "axes[0, 0].set_xlabel('Epoch')\n",
    "axes[0, 0].set_ylabel('Loss')\n",
    "axes[0, 0].legend()\n",
    "axes[0, 0].grid(True)\n",
    "\n",
    "# Accuracy plot\n",
    "axes[0, 1].plot(epochs, history['train_acc'], 'b-', label='Train')\n",
    "axes[0, 1].plot(epochs, history['val_acc'], 'r-', label='Val Top-1')\n",
    "axes[0, 1].plot(epochs, history['val_acc_top5'], 'g-', label='Val Top-5')\n",
    "axes[0, 1].set_title('Accuracy')\n",
    "axes[0, 1].set_xlabel('Epoch')\n",
    "axes[0, 1].set_ylabel('Accuracy (%)')\n",
    "axes[0, 1].legend()\n",
    "axes[0, 1].grid(True)\n",
    "\n",
    "# Learning rate plot\n",
    "axes[1, 0].plot(epochs, history['lr'], 'orange')\n",
    "axes[1, 0].set_title('Learning Rate')\n",
    "axes[1, 0].set_xlabel('Epoch')\n",
    "axes[1, 0].set_ylabel('LR')\n",
    "axes[1, 0].set_yscale('log')\n",
    "axes[1, 0].grid(True)\n",
    "\n",
    "# Summary text\n",
    "axes[1, 1].axis('off')\n",
    "summary_text = f\"\"\"Training Summary:\n",
    "\n",
    "Model: ResNet50\n",
    "Batch Size: {params.batch_size}\n",
    "Epochs Trained: {len(history['train_loss'])}\n",
    "\n",
    "Best Val Acc: {best_val_acc:.2f}%\n",
    "Best Epoch: {best_epoch + 1}\n",
    "\n",
    "Final Train Acc: {history['train_acc'][-1]:.2f}%\n",
    "Final Val Acc: {history['val_acc'][-1]:.2f}%\n",
    "Final Val Top-5: {history['val_acc_top5'][-1]:.2f}%\n",
    "\"\"\"\n",
    "axes[1, 1].text(0.1, 0.5, summary_text, fontsize=12, \n",
    "                verticalalignment='center', fontfamily='monospace')\n",
    "\n",
    "plt.suptitle('ResNet50 Training Progress', fontsize=16)\n",
    "plt.tight_layout()\n",
    "\n",
    "# Save the plot\n",
    "Path(\"plots\").mkdir(exist_ok=True)\n",
    "plt.savefig(\"plots/resnet50_training_curves.png\", dpi=100)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Save Training History\n",
    "# Save training history as JSON\n",
    "Path(\"logs\").mkdir(exist_ok=True)\n",
    "\n",
    "history_path = \"logs/resnet50_training_history.json\"\n",
    "with open(history_path, 'w') as f:\n",
    "    json.dump(history, f, indent=4)\n",
    "print(f\"Training history saved to {history_path}\")\n",
    "\n",
    "# Save training configuration\n",
    "config_dict = {\n",
    "    'model': 'ResNet50',\n",
    "    'batch_size': params.batch_size,\n",
    "    'initial_lr': params.lr,\n",
    "    'momentum': params.momentum,\n",
    "    'weight_decay': params.weight_decay,\n",
    "    'epochs_trained': len(history['train_loss']),\n",
    "    'best_val_acc': best_val_acc,\n",
    "    'best_epoch': best_epoch + 1\n",
    "}\n",
    "\n",
    "config_path = \"logs/resnet50_config.json\"\n",
    "with open(config_path, 'w') as f:\n",
    "    json.dump(config_dict, f, indent=4)\n",
    "print(f\"Configuration saved to {config_path}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
